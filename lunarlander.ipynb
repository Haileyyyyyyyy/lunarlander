{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"id":"J5feuj4FZPzm"},"source":["# (A3) 빅데이터 혁신공유대학 여름캠프\n","> 강화학습, LunarLander\n","\n","- toc:true\n","- branch: master\n","- badges: true\n","- comments: true\n","- author: 최규빈, 고한규"]},{"cell_type":"markdown","metadata":{"id":"HingAhTBZPzo"},"source":["***본 강의노트는 LG전자 인공지능연구소 고한규박사님께 공유받았습니다.***"]},{"cell_type":"markdown","metadata":{"id":"W4f3kZCsuYym"},"source":["## 1. Video Record 및 Play 를 위해 필요한 설치 패키지 추가"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"LplxBeVPejAR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660619361419,"user_tz":-540,"elapsed":48829,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}},"outputId":"490d9b15-6377-4215-fa4d-145867a58fde"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 177 kB 8.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 63.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.5 MB 52.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 64.4 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 448 kB 8.2 MB/s \n","\u001b[K     |████████████████████████████████| 67 kB 4.1 MB/s \n","\u001b[?25h"]}],"source":["!sudo apt-get update > /dev/null 2>&1\n","!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install rarfile --quiet\n","!pip install stable-baselines3[extra] ale-py==0.7.4 --quiet\n","!pip install box2d-py --quiet\n","!pip install gym pyvirtualdisplay --quiet\n","!pip install pyglet --quiet\n","!pip install piglet --quiet"]},{"cell_type":"markdown","metadata":{"id":"mXZ3lt8LulwU"},"source":["## 2. Virtual Display"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"RQOamIJDemX2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660619361906,"user_tz":-540,"elapsed":493,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}},"outputId":"078970a9-c225-4ba0-eeb6-5cbf900e4f8e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7fe951983350>"]},"metadata":{},"execution_count":10}],"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"]},{"cell_type":"markdown","metadata":{"id":"dFp6mjSFuvUu"},"source":["## 3. LunarLander "]},{"cell_type":"code","execution_count":11,"metadata":{"id":"gleN5iPNe76N","executionInfo":{"status":"ok","timestamp":1660619364916,"user_tz":-540,"elapsed":3013,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["import gym\n","from gym.wrappers import Monitor\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import base64, io\n","\n","import numpy as np\n","from collections import deque, namedtuple\n","\n","# For visualization\n","from gym.wrappers.monitoring import video_recorder\n","from IPython.display import HTML\n","from IPython import display \n","import glob"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"o8o1VH2RfJfS","executionInfo":{"status":"ok","timestamp":1660619364916,"user_tz":-540,"elapsed":20,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0063a849-700c-4e3a-caed-c19f21de0652"},"outputs":[{"output_type":"stream","name":"stdout","text":["State shape:  (8,)\n","Number of actions:  4\n"]}],"source":["env = gym.make('LunarLander-v2')\n","env.seed(0)\n","print('State shape: ', env.observation_space.shape)\n","print('Number of actions: ', env.action_space.n)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"i7evjzqsfLn4","executionInfo":{"status":"ok","timestamp":1660619364916,"user_tz":-540,"elapsed":17,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["class QNetwork(nn.Module):\n","    \"\"\"Actor (Policy) Model.\"\"\"\n","\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize parameters and build model.\n","        Params\n","        ======\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","        \"\"\"\n","        super(QNetwork, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.layer1 = nn.Linear(state_size, 128)\n","        self.layer2 = nn.Linear(128, 64)\n","        self.layer3 = nn.Linear(64, 32)\n","        self.layer4 = nn.Linear(32, action_size)\n","        \n","    def forward(self, state):\n","        \"\"\"Build a network that maps state -> action values.\"\"\"\n","        x = F.relu(self.layer1(state))        \n","        x = F.relu(self.layer2(x))\n","        x = F.relu(self.layer3(x))\n","        x = self.layer4(x)\n","        return x"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"XA89PKLwfPIL","executionInfo":{"status":"ok","timestamp":1660619364916,"user_tz":-540,"elapsed":16,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["BUFFER_SIZE = 50000   # replay buffer size\n","BATCH_SIZE = 32     # minibatch size\n","GAMMA = 0.99          # discount factor\n","TAU = 0.001           # for soft update of target parameters\n","LR = 0.0003           # learning rate \n","UPDATE_EVERY = 1      # how often to update the network"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"n2mHCirFfSTN","executionInfo":{"status":"ok","timestamp":1660619364917,"user_tz":-540,"elapsed":17,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95e4b674-e7b6-474b-8496-a3c4f0d6e002"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":15}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"1lP2lvSufVJr","executionInfo":{"status":"ok","timestamp":1660619364917,"user_tz":-540,"elapsed":13,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["class ReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","\n","    def __init__(self, action_size, buffer_size, batch_size, seed):\n","        \"\"\"Initialize a ReplayBuffer object.\n","\n","        Params\n","        ======\n","            action_size (int): dimension of each action\n","            buffer_size (int): maximum size of buffer\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","        \"\"\"\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=buffer_size)  \n","        self.batch_size = batch_size\n","        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","        self.seed = random.seed(seed)\n","    \n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        e = self.experience(state, action, reward, next_state, done)\n","        self.memory.append(e)\n","    \n","    def sample(self):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        experiences = random.sample(self.memory, k=self.batch_size)\n","        \n","        # Convert to torch tensors\n","        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n","        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n","        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n","        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n","        \n","        # Convert done from boolean to int\n","        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)        \n","        \n","        return (states, actions, rewards, next_states, dones)\n","\n","    def __len__(self):\n","        \"\"\"Return the current size of internal memory.\"\"\"\n","        return len(self.memory)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"H_czW7AifX1P","executionInfo":{"status":"ok","timestamp":1660619364917,"user_tz":-540,"elapsed":13,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["class Agent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize an Agent object.\n","        \n","        Params\n","        ======\n","            state_size (int): dimension of each state\n","            action_size (int): dimension of each action\n","            seed (int): random seed\n","        \"\"\"\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.seed = random.seed(seed)\n","\n","        # Q-Network\n","        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n","        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n","        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n","\n","        # Replay memory\n","        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n","        # Initialize time step (for updating every UPDATE_EVERY steps)\n","        self.t_step = 0\n","    \n","    def step(self, state, action, reward, next_state, done):\n","        # Save experience in replay memory\n","        self.memory.add(state, action, reward, next_state, done)\n","        \n","        # Learn every UPDATE_EVERY time steps.\n","        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n","        if self.t_step == 0:\n","            # If enough samples are available in memory, get random subset and learn\n","            if len(self.memory) > BATCH_SIZE:\n","                experiences = self.memory.sample()\n","                self.learn(experiences, GAMMA)\n","\n","    def act(self, state, eps=0.):\n","        \"\"\"Returns actions for given state as per current policy.\n","        \n","        Params\n","        ======\n","            state (array_like): current state\n","            eps (float): epsilon, for epsilon-greedy action selection\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        \n","        \n","        self.qnetwork_local.eval()\n","        with torch.no_grad():\n","            action_values = self.qnetwork_local(state)            \n","        self.qnetwork_local.train()\n","\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","        \n","\n","    def learn(self, experiences, gamma):\n","        \"\"\"Update value parameters using given batch of experience tuples.\n","\n","        Params\n","        ======\n","            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n","            gamma (float): discount factor\n","        \"\"\"\n","        # Obtain random minibatch of tuples from D\n","        states, actions, rewards, next_states, dones = experiences\n","\n","        ## Compute and minimize the loss\n","        ### Extract next maximum estimated value from target network\n","        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n","        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n","\n","        ### Calculate expected value from local network\n","        q_expected = self.qnetwork_local(states).gather(1, actions)\n","        \n","        loss = F.mse_loss(q_expected, q_targets)\n","        \n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # ------------------- update target network ------------------- #\n","        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n","\n","    def soft_update(self, local_model, target_model, tau):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","\n","        Params\n","        ======\n","            local_model (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","            tau (float): interpolation parameter \n","        \"\"\"\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"phRDoLijfaEK","executionInfo":{"status":"ok","timestamp":1660619364917,"user_tz":-540,"elapsed":12,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["def dqn(n_episodes=2900, max_t=500, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n","    \"\"\"Deep Q-Learning.\n","    \n","    Params\n","    ======\n","        n_episodes (int): maximum number of training episodes\n","        max_t (int): maximum number of timesteps per episode\n","        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n","        eps_end (float): minimum value of epsilon\n","        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n","    \"\"\"\n","    playtimes=[] \n","    scores = []                        # list containing scores from each episode\n","    scores_window = deque(maxlen=100)  # last 100 scores\n","    eps = eps_start                    # initialize epsilon\n","    for i_episode in range(1, n_episodes+1):\n","        state = env.reset()\n","        score = 0\n","        for t in range(max_t):\n","            ## STEP1: \n","            action = agent.act(state, eps)\n","            \n","            ## STEP2: \n","            next_state, reward, done, _ = env.step(action)\n","            \n","            ## STEP3: \n","            agent.step(state, action, reward, next_state, done)\n","            \n","            ## STEP4: \n","            state = next_state\n","            \n","            ## STEP5: \n","            score += reward\n","            \n","            if done:\n","                playtimes.append(t)\n","                break \n","        scores_window.append(score)       # save most recent score\n","        scores.append(score)              # save most recent score\n","        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n","        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tPlaytime: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(playtimes)), end=\"\")\n","        if i_episode % 100 == 0:\n","            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tPlaytime: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(playtimes)))\n","            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n","        if np.mean(scores_window)>=290.0:\n","            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n","            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n","            break\n","    return scores"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"rtmdkj6iZPzu","executionInfo":{"status":"ok","timestamp":1660619364918,"user_tz":-540,"elapsed":13,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["# agent = Agent(state_size=8, action_size=4, seed=0)\n","# scores = dqn()"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"I_aZeiLUfcQy","executionInfo":{"status":"ok","timestamp":1660619364918,"user_tz":-540,"elapsed":13,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["# plot the scores\n","# print(sum(scores[2800:2900])/100)\n","# fig = plt.figure()\n","# ax = fig.add_subplot(111)\n","# plt.plot(np.arange(len(scores)), scores)\n","# plt.ylabel('Score')\n","# plt.xlabel('Episode #')\n","# plt.show()"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"maRp4yRTffaW","executionInfo":{"status":"ok","timestamp":1660619364918,"user_tz":-540,"elapsed":13,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["def show_video(env_name):\n","    mp4list = glob.glob('video/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","def wrap_env(env):\n","    env = Monitor(env, './video', force=True)\n","    return env\n","\n","def gen_wrapped_env(env_name):\n","    return wrap_env(gym.make(env_name))\n","        \n","def show_video_of_model(agent, env_name):\n","    env = gen_wrapped_env(env_name)\n","    agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n","    state = env.reset()\n","    done = False\n","    while not done:\n","        frame = env.render(mode='rgb_array')\n","        \n","        action = agent.act(state)\n","\n","        state, reward, done, _ = env.step(action)        \n","    env.close()"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"nTuXsDwkfhuC","executionInfo":{"status":"error","timestamp":1660619368909,"user_tz":-540,"elapsed":4001,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}},"colab":{"base_uri":"https://localhost:8080/","height":337},"outputId":"76edab6b-af77-43d1-b3e6-df0e0dea07e8"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b4fbd53cdbbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshow_video_of_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-65e1c083bf53>\u001b[0m in \u001b[0;36mshow_video_of_model\u001b[0;34m(agent, env_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_video_of_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_wrapped_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint.pth'"]}],"source":["agent = Agent(state_size=8, action_size=4, seed=0)\n","show_video_of_model(agent, 'LunarLander-v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjGYCgc5g3yD","executionInfo":{"status":"aborted","timestamp":1660619368909,"user_tz":-540,"elapsed":6,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"outputs":[],"source":["# show_video('LunarLander-v2')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"u7xJG9DJmnrD","executionInfo":{"status":"aborted","timestamp":1660619368909,"user_tz":-540,"elapsed":6,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Github/lunarlander/"],"metadata":{"id":"ZFs6CxtbmqBu","executionInfo":{"status":"aborted","timestamp":1660619368909,"user_tz":-540,"elapsed":6,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git config --global user.email 'rlaekdus17@sookmyung.ac.kr'\n","!git config --global user.name 'Haileyyyyyyyy'"],"metadata":{"id":"SwH8Mq3qnCum","executionInfo":{"status":"aborted","timestamp":1660619368910,"user_tz":-540,"elapsed":7,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add lunarlander.ipynb\n","!git commit -m \"Connect Google Colab and Drive\""],"metadata":{"id":"zpSmKw99nE7K","executionInfo":{"status":"aborted","timestamp":1660619368910,"user_tz":-540,"elapsed":7,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"cDDYs_79nNro","executionInfo":{"status":"aborted","timestamp":1660619368910,"user_tz":-540,"elapsed":7,"user":{"displayName":"Dayeon Kim","userId":"15498695463058839868"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"lunarlander.ipynb","provenance":[{"file_id":"https://github.com/guebin/BDA2021/blob/master/_notebooks/2022-08-08-(A3)%20lunarlander_render.ipynb","timestamp":1660045130926}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}